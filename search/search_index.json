{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to WAGGON: WAssrestein Global Gradient-free OptimisatioN WAGGON is a python library of black box gradient-free optimisation. Currently, the library contains implementations of optimisation methods based on Wasserstein uncertainty and baseline approaches from the following papers: Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv] Implemented methods Wasserstein Uncertainty Global Optimisation (WU-GO) Bayesian optimisation: via Expected Improvement (EI), Lower and Upper Confidence Bounds (LCB, UCB) Installation pip install waggon or git clone https://github.com/hse-cs/waggon cd waggon pip install -e Basic usage (See more examples in the documentation .) The following code snippet (does this and that) import waggon from waggon.optim import Optimiser from waggon.acquisitions import WU from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = Optimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise waggon.utils.display() Support Home: https://github.com/hse-cs/waggon Documentation: https://hse-cs.github.io/waggon For any usage questions, suggestions and bugs please use the issue page .","title":"Home"},{"location":"#welcome-to-waggon-wassrestein-global-gradient-free-optimisation","text":"WAGGON is a python library of black box gradient-free optimisation. Currently, the library contains implementations of optimisation methods based on Wasserstein uncertainty and baseline approaches from the following papers: Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv]","title":"Welcome to WAGGON: WAssrestein Global Gradient-free OptimisatioN"},{"location":"#implemented-methods","text":"Wasserstein Uncertainty Global Optimisation (WU-GO) Bayesian optimisation: via Expected Improvement (EI), Lower and Upper Confidence Bounds (LCB, UCB)","title":"Implemented methods"},{"location":"#installation","text":"pip install waggon or git clone https://github.com/hse-cs/waggon cd waggon pip install -e","title":"Installation"},{"location":"#basic-usage","text":"(See more examples in the documentation .) The following code snippet (does this and that) import waggon from waggon.optim import Optimiser from waggon.acquisitions import WU from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = Optimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise waggon.utils.display()","title":"Basic usage"},{"location":"#support","text":"Home: https://github.com/hse-cs/waggon Documentation: https://hse-cs.github.io/waggon For any usage questions, suggestions and bugs please use the issue page .","title":"Support"},{"location":"about/","text":"Black-box Optimisation Simulation of real-world experiments is key to scientific discoveries and engineering solutions. Such techniques use parameters describing configurations and architectures of the system. A common challenge is to find the optimal configuration for some objective, e.g., such that it maximises efficiency or minimises costs and overhead. The simulator is treated as a black box experiment. This means that observations come from an unknown and likely difficult-to-estimate function. Using a surrogate model for black-box optimisation (BBO) is an established technique, as BBO has a rich history of both gradient and gradient-free methods, most of which come from tackling problems that arise in physics, chemistry and engineering. The optimisation problem can be defined as: $$ \\inf_{\\theta \\in \\Theta} f(\\theta). $$ If the black-box function is stochastic, the problem accepts the following form: $$ \\inf_{\\theta \\in \\Theta} \\mathbb{E}\\left[ f(\\theta, x) \\right]. $$ And in case of distributionally robust optimistion, the objective becomes: $$ \\inf_{\\theta \\in \\Theta} \\sup_{\\mu \\in \\mathcal{P}} \\mathbb{E}_{x \\sim \\mu} \\left[ f(\\theta, x) \\right], $$ where $\\mathcal{P}$ is an ambiguity set, i.e., set of viable distributions.","title":"Black-box optimisation"},{"location":"about/#black-box-optimisation","text":"Simulation of real-world experiments is key to scientific discoveries and engineering solutions. Such techniques use parameters describing configurations and architectures of the system. A common challenge is to find the optimal configuration for some objective, e.g., such that it maximises efficiency or minimises costs and overhead. The simulator is treated as a black box experiment. This means that observations come from an unknown and likely difficult-to-estimate function. Using a surrogate model for black-box optimisation (BBO) is an established technique, as BBO has a rich history of both gradient and gradient-free methods, most of which come from tackling problems that arise in physics, chemistry and engineering. The optimisation problem can be defined as: $$ \\inf_{\\theta \\in \\Theta} f(\\theta). $$ If the black-box function is stochastic, the problem accepts the following form: $$ \\inf_{\\theta \\in \\Theta} \\mathbb{E}\\left[ f(\\theta, x) \\right]. $$ And in case of distributionally robust optimistion, the objective becomes: $$ \\inf_{\\theta \\in \\Theta} \\sup_{\\mu \\in \\mathcal{P}} \\mathbb{E}_{x \\sim \\mu} \\left[ f(\\theta, x) \\right], $$ where $\\mathcal{P}$ is an ambiguity set, i.e., set of viable distributions.","title":"Black-box Optimisation"},{"location":"bnn/","text":"TBD","title":"Bayesian Neural Networks"},{"location":"cb/","text":"TBD","title":"Confidence Bound"},{"location":"custom_acqf/","text":"TBD","title":"Custom Acquisition Functions"},{"location":"custom_exp/","text":"TBD","title":"Custom experiments"},{"location":"custom_surr/","text":"TBD","title":"Custom Surrogates"},{"location":"de/","text":"TBD","title":"Deep Adversarial Ensembles"},{"location":"dgm/","text":"TBD","title":"Deep Generative Surrogates"},{"location":"dgp/","text":"TBD","title":"Deep Gaussian Processes"},{"location":"ei/","text":"TBD","title":"Expected Improvement"},{"location":"functions/","text":"TBD","title":"Test functions"},{"location":"gp/","text":"TBD","title":"Gaussian Processes"},{"location":"optim/","text":"TBD","title":"Optimiser"},{"location":"surr_opt/","text":"TBD","title":"Surrogate based optimisation"},{"location":"wu/","text":"TBD","title":"Wasserstein Uncertainty"}]}