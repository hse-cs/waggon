{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to WAGGON: WAssrestein Global Gradient-free OptimisatioN WAGGON is a python library of black box gradient-free optimisation. Currently, the library contains implementations of optimisation methods based on Wasserstein uncertainty and baseline approaches from the following papers: Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv] Implemented methods Wasserstein Uncertainty Global Optimisation (WU-GO) Bayesian optimisation: via Expected Improvement (EI), Lower and Upper Confidence Bounds (LCB, UCB) Installation pip install waggon or git clone https://github.com/hse-cs/waggon cd waggon pip install -e Basic usage (See more examples in the documentation .) The following code snippet (does this and that) import waggon from waggon.acquisitions import WU from waggon.optim import SurrogateOptimiser from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display() Support Home: https://github.com/hse-cs/waggon Documentation: https://hse-cs.github.io/waggon For any usage questions, suggestions and bugs please use the issue page .","title":"Home"},{"location":"#welcome-to-waggon-wassrestein-global-gradient-free-optimisation","text":"WAGGON is a python library of black box gradient-free optimisation. Currently, the library contains implementations of optimisation methods based on Wasserstein uncertainty and baseline approaches from the following papers: Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv]","title":"Welcome to WAGGON: WAssrestein Global Gradient-free OptimisatioN"},{"location":"#implemented-methods","text":"Wasserstein Uncertainty Global Optimisation (WU-GO) Bayesian optimisation: via Expected Improvement (EI), Lower and Upper Confidence Bounds (LCB, UCB)","title":"Implemented methods"},{"location":"#installation","text":"pip install waggon or git clone https://github.com/hse-cs/waggon cd waggon pip install -e","title":"Installation"},{"location":"#basic-usage","text":"(See more examples in the documentation .) The following code snippet (does this and that) import waggon from waggon.acquisitions import WU from waggon.optim import SurrogateOptimiser from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display()","title":"Basic usage"},{"location":"#support","text":"Home: https://github.com/hse-cs/waggon Documentation: https://hse-cs.github.io/waggon For any usage questions, suggestions and bugs please use the issue page .","title":"Support"},{"location":"about/","text":"Black-box Optimisation Simulation of real-world experiments is key to scientific discoveries and engineering solutions. Such techniques use parameters describing configurations and architectures of the system. A common challenge is to find the optimal configuration for some objective, e.g., such that it maximises efficiency or minimises costs and overhead. The simulator is treated as a black box experiment. This means that observations come from an unknown and likely difficult-to-estimate function. Using a surrogate model for black-box optimisation (BBO) is an established technique, as BBO has a rich history of both gradient and gradient-free methods, most of which come from tackling problems that arise in physics, chemistry and engineering. The optimisation problem can be defined as: $$ \\inf_{\\theta \\in \\Theta} f(\\theta), $$ where $\\Theta$ is the search space. If the black-box function is stochastic, the problem accepts the following form: $$ \\inf_{\\theta \\in \\Theta} \\mathbb{E}\\left[ f(\\theta, x) \\right]. $$ And in case of distributionally robust optimistion, the objective becomes: $$ \\inf_{\\theta \\in \\Theta} \\sup_{\\mu \\in \\mathcal{P}} \\mathbb{E}_{x \\sim \\mu} \\left[ f(\\theta, x) \\right], $$ where $\\mathcal{P}$ is an ambiguity set, i.e., set of viable distributions.","title":"Black-box optimisation"},{"location":"about/#black-box-optimisation","text":"Simulation of real-world experiments is key to scientific discoveries and engineering solutions. Such techniques use parameters describing configurations and architectures of the system. A common challenge is to find the optimal configuration for some objective, e.g., such that it maximises efficiency or minimises costs and overhead. The simulator is treated as a black box experiment. This means that observations come from an unknown and likely difficult-to-estimate function. Using a surrogate model for black-box optimisation (BBO) is an established technique, as BBO has a rich history of both gradient and gradient-free methods, most of which come from tackling problems that arise in physics, chemistry and engineering. The optimisation problem can be defined as: $$ \\inf_{\\theta \\in \\Theta} f(\\theta), $$ where $\\Theta$ is the search space. If the black-box function is stochastic, the problem accepts the following form: $$ \\inf_{\\theta \\in \\Theta} \\mathbb{E}\\left[ f(\\theta, x) \\right]. $$ And in case of distributionally robust optimistion, the objective becomes: $$ \\inf_{\\theta \\in \\Theta} \\sup_{\\mu \\in \\mathcal{P}} \\mathbb{E}_{x \\sim \\mu} \\left[ f(\\theta, x) \\right], $$ where $\\mathcal{P}$ is an ambiguity set, i.e., set of viable distributions.","title":"Black-box Optimisation"},{"location":"bnn/","text":"Description Bayesian inference allows us to learn a probability distribution over possible neural networks. By making a simple adjustment to standard neural network methods, we can approximately solve this inference problem. The resulting approach reduces overfitting, facilitates learning from small datasets, and provides insights into the uncertainty of our predictions. In Bayesian inference, instead of relying on a single point estimate of the weights, $w^ $, and its associated prediction function, $\\hat{y}_{w^ }(x)$, as in conventional training, we infer distributions $p(w|D)$ and $p(\\hat{y}_{w^*}(x)|D)$. One key advantage of using distributions for model parameters and predictions is the ability to quantify uncertainty, such as by calculating the variance [3]. Usage from waggon.surrogates import BNN [3] Xu, W., Ricky, Li, X. and Duvenaud, D. (2022). Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations. PMLR, pp.721\u2013738. Available online . \u200c","title":"Bayesian Neural Networks"},{"location":"bnn/#description","text":"Bayesian inference allows us to learn a probability distribution over possible neural networks. By making a simple adjustment to standard neural network methods, we can approximately solve this inference problem. The resulting approach reduces overfitting, facilitates learning from small datasets, and provides insights into the uncertainty of our predictions. In Bayesian inference, instead of relying on a single point estimate of the weights, $w^ $, and its associated prediction function, $\\hat{y}_{w^ }(x)$, as in conventional training, we infer distributions $p(w|D)$ and $p(\\hat{y}_{w^*}(x)|D)$. One key advantage of using distributions for model parameters and predictions is the ability to quantify uncertainty, such as by calculating the variance [3].","title":"Description"},{"location":"bnn/#usage","text":"from waggon.surrogates import BNN [3] Xu, W., Ricky, Li, X. and Duvenaud, D. (2022). Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations. PMLR, pp.721\u2013738. Available online . \u200c","title":"Usage"},{"location":"cb/","text":"TBD","title":"Confidence Bound"},{"location":"custom_acqf/","text":"TBD","title":"Custom Acquisition Functions"},{"location":"custom_exp/","text":"TBD","title":"Custom experiments"},{"location":"custom_surr/","text":"TBD","title":"Custom Surrogates"},{"location":"de/","text":"TBD","title":"Deep Adversarial Ensembles"},{"location":"dgm/","text":"Description Having a generator model that would map a simple distribution, $\\mathcal{Z}$, to a complex and unknown distribution, $p(\\mu)$, is desired in many settings as it allows for the generation of samples from the intractable data space. For our purposes, conditional deep generative models (DGMs) are used to model the stochastic response of the simulator. This allows to model essentially any response shape. The goal of a conditional DGM is to obtain a generator $G: \\mathbb{R}^s \\times \\Theta \\rightarrow \\mathbb{R}^d$ such that distributions $G(\\mathcal{Z}; \\theta)$ and $p(\\mu; \\theta)$ match for each $\\theta \\in \\Theta$. Since $\\mathcal{Z}$ and $p(\\mu)$ are independent, we get $G(\\mathcal{Z}; \\theta) \\sim p(\\mu; \\theta)$. For simplicity of notation we denote $G(\\mathcal{Z}; \\theta)$ as $G(\\theta)$. As per [2], the package contains the implementation of a Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) to be used with Wasserstein uncertainty. Usage from waggon.surrogates.gan import WGAN [2] Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv]","title":"Deep Generative Surrogates"},{"location":"dgm/#description","text":"Having a generator model that would map a simple distribution, $\\mathcal{Z}$, to a complex and unknown distribution, $p(\\mu)$, is desired in many settings as it allows for the generation of samples from the intractable data space. For our purposes, conditional deep generative models (DGMs) are used to model the stochastic response of the simulator. This allows to model essentially any response shape. The goal of a conditional DGM is to obtain a generator $G: \\mathbb{R}^s \\times \\Theta \\rightarrow \\mathbb{R}^d$ such that distributions $G(\\mathcal{Z}; \\theta)$ and $p(\\mu; \\theta)$ match for each $\\theta \\in \\Theta$. Since $\\mathcal{Z}$ and $p(\\mu)$ are independent, we get $G(\\mathcal{Z}; \\theta) \\sim p(\\mu; \\theta)$. For simplicity of notation we denote $G(\\mathcal{Z}; \\theta)$ as $G(\\theta)$. As per [2], the package contains the implementation of a Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) to be used with Wasserstein uncertainty.","title":"Description"},{"location":"dgm/#usage","text":"from waggon.surrogates.gan import WGAN [2] Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv]","title":"Usage"},{"location":"dgp/","text":"TBD","title":"Deep Gaussian Processes"},{"location":"ei/","text":"TBD","title":"Expected Improvement"},{"location":"functions/","text":"TBD","title":"Test functions"},{"location":"gp/","text":"TBD","title":"Gaussian Processes"},{"location":"optim/","text":"Description WAGGON focuses on optimisation methods, and Optimiser is a base class for optimisation algorithms. It contains common methods and properties, e.g., optimise that runs the optimisation loop until the chosen error, error_type , is small enough, opt_eps , and create_candidates that samples candidate points using Latin Hyperube sampling. The class can be inherited for implementing specific approaches. Currently waggon.optim contains surrogate-based optimisation, SurrogateOptimiser (described in the following section ). New methods will be added as our research continues and can be suggested via a pull request. Usage from waggon.optim import Optimiser","title":"Optimiser"},{"location":"optim/#description","text":"WAGGON focuses on optimisation methods, and Optimiser is a base class for optimisation algorithms. It contains common methods and properties, e.g., optimise that runs the optimisation loop until the chosen error, error_type , is small enough, opt_eps , and create_candidates that samples candidate points using Latin Hyperube sampling. The class can be inherited for implementing specific approaches. Currently waggon.optim contains surrogate-based optimisation, SurrogateOptimiser (described in the following section ). New methods will be added as our research continues and can be suggested via a pull request.","title":"Description"},{"location":"optim/#usage","text":"from waggon.optim import Optimiser","title":"Usage"},{"location":"surr_opt/","text":"Description Surrogate-based optimisation (SBO) carries out optimisation using a surrogate model. SBO does not strive for global accuracy. Instead it only requires the surrogate model to be sufficiently accurate to guide optimisation toward the true optimum. SBO can be particularly useful in several scenarious. For example, when the original model is computationally expensive, or when data is noise, or both [1]. SurrogateOptimiser is based on the Optimiser class. To run, it requires the experiment, waggon.Function , the surrogate model, waggon.Surrogate , and the acquisition function, waggon.acquisition . Usage import waggon from waggon.acquisitions import WU from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel from waggon.optim import SurrogateOptimiser # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display() [1] Martins, J.R.R.A. and Ning, S.A. (2022) Surrogate-based Optimisation in Engineering design optimization . Cambridge, United Kingdom: Cambridge University Press.","title":"Surrogate based optimisation"},{"location":"surr_opt/#description","text":"Surrogate-based optimisation (SBO) carries out optimisation using a surrogate model. SBO does not strive for global accuracy. Instead it only requires the surrogate model to be sufficiently accurate to guide optimisation toward the true optimum. SBO can be particularly useful in several scenarious. For example, when the original model is computationally expensive, or when data is noise, or both [1]. SurrogateOptimiser is based on the Optimiser class. To run, it requires the experiment, waggon.Function , the surrogate model, waggon.Surrogate , and the acquisition function, waggon.acquisition .","title":"Description"},{"location":"surr_opt/#usage","text":"import waggon from waggon.acquisitions import WU from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel from waggon.optim import SurrogateOptimiser # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display() [1] Martins, J.R.R.A. and Ning, S.A. (2022) Surrogate-based Optimisation in Engineering design optimization . Cambridge, United Kingdom: Cambridge University Press.","title":"Usage"},{"location":"wu/","text":"TBD","title":"Wasserstein Uncertainty"}]}