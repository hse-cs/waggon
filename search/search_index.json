{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to WAGGON: WAssrestein Global Gradient-free OptimisatioN WAGGON is a python library of black box gradient-free optimisation. Currently, the library contains implementations of optimisation methods based on Wasserstein uncertainty and baseline approaches from the following papers: Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv] Implemented methods Wasserstein Uncertainty Global Optimisation (WU-GO) Bayesian optimisation: via Expected Improvement (EI), Lower and Upper Confidence Bounds (LCB, UCB) Installation pip install waggon or git clone https://github.com/hse-cs/waggon cd waggon pip install -e Basic usage (See more examples in the documentation .) The following code snippet (does this and that) import waggon from waggon.acquisitions import WU from waggon.optim import SurrogateOptimiser from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display() Support Home: https://github.com/hse-cs/waggon Documentation: https://hse-cs.github.io/waggon For any usage questions, suggestions and bugs please use the issue page .","title":"Home"},{"location":"#welcome-to-waggon-wassrestein-global-gradient-free-optimisation","text":"WAGGON is a python library of black box gradient-free optimisation. Currently, the library contains implementations of optimisation methods based on Wasserstein uncertainty and baseline approaches from the following papers: Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv]","title":"Welcome to WAGGON: WAssrestein Global Gradient-free OptimisatioN"},{"location":"#implemented-methods","text":"Wasserstein Uncertainty Global Optimisation (WU-GO) Bayesian optimisation: via Expected Improvement (EI), Lower and Upper Confidence Bounds (LCB, UCB)","title":"Implemented methods"},{"location":"#installation","text":"pip install waggon or git clone https://github.com/hse-cs/waggon cd waggon pip install -e","title":"Installation"},{"location":"#basic-usage","text":"(See more examples in the documentation .) The following code snippet (does this and that) import waggon from waggon.acquisitions import WU from waggon.optim import SurrogateOptimiser from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display()","title":"Basic usage"},{"location":"#support","text":"Home: https://github.com/hse-cs/waggon Documentation: https://hse-cs.github.io/waggon For any usage questions, suggestions and bugs please use the issue page .","title":"Support"},{"location":"about/","text":"Black-box Optimisation Simulation of real-world experiments is key to scientific discoveries and engineering solutions. Such techniques use parameters describing configurations and architectures of the system. A common challenge is to find the optimal configuration for some objective, e.g., such that it maximises efficiency or minimises costs and overhead. The simulator is treated as a black box experiment. This means that observations come from an unknown and likely difficult-to-estimate function. Using a surrogate model for black-box optimisation (BBO) is an established technique, as BBO has a rich history of both gradient and gradient-free methods, most of which come from tackling problems that arise in physics, chemistry and engineering. The optimisation problem can be defined as: $$ \\inf_{\\theta \\in \\Theta} f(\\theta), $$ where $\\Theta$ is the search space. If the black-box function is stochastic, the problem accepts the following form: $$ \\inf_{\\theta \\in \\Theta} \\mathbb{E}\\left[ f(\\theta, x) \\right]. $$ And in case of distributionally robust optimistion, the objective becomes: $$ \\inf_{\\theta \\in \\Theta} \\sup_{\\mu \\in \\mathcal{P}} \\mathbb{E}_{x \\sim \\mu} \\left[ f(\\theta, x) \\right], $$ where $\\mathcal{P}$ is an ambiguity set, i.e., set of viable distributions.","title":"Black-box optimisation"},{"location":"about/#black-box-optimisation","text":"Simulation of real-world experiments is key to scientific discoveries and engineering solutions. Such techniques use parameters describing configurations and architectures of the system. A common challenge is to find the optimal configuration for some objective, e.g., such that it maximises efficiency or minimises costs and overhead. The simulator is treated as a black box experiment. This means that observations come from an unknown and likely difficult-to-estimate function. Using a surrogate model for black-box optimisation (BBO) is an established technique, as BBO has a rich history of both gradient and gradient-free methods, most of which come from tackling problems that arise in physics, chemistry and engineering. The optimisation problem can be defined as: $$ \\inf_{\\theta \\in \\Theta} f(\\theta), $$ where $\\Theta$ is the search space. If the black-box function is stochastic, the problem accepts the following form: $$ \\inf_{\\theta \\in \\Theta} \\mathbb{E}\\left[ f(\\theta, x) \\right]. $$ And in case of distributionally robust optimistion, the objective becomes: $$ \\inf_{\\theta \\in \\Theta} \\sup_{\\mu \\in \\mathcal{P}} \\mathbb{E}_{x \\sim \\mu} \\left[ f(\\theta, x) \\right], $$ where $\\mathcal{P}$ is an ambiguity set, i.e., set of viable distributions.","title":"Black-box Optimisation"},{"location":"bnn/","text":"TBD","title":"Bayesian Neural Networks"},{"location":"cb/","text":"TBD","title":"Confidence Bound"},{"location":"custom_acqf/","text":"TBD","title":"Custom Acquisition Functions"},{"location":"custom_exp/","text":"TBD","title":"Custom experiments"},{"location":"custom_surr/","text":"TBD","title":"Custom Surrogates"},{"location":"de/","text":"TBD","title":"Deep Adversarial Ensembles"},{"location":"dgm/","text":"TBD","title":"Deep Generative Surrogates"},{"location":"dgp/","text":"TBD","title":"Deep Gaussian Processes"},{"location":"ei/","text":"TBD","title":"Expected Improvement"},{"location":"functions/","text":"TBD","title":"Test functions"},{"location":"gp/","text":"TBD","title":"Gaussian Processes"},{"location":"optim/","text":"Description WAGGON focuses on optimisation methods, and Optimiser is a base class for optimisation algorithms. It contains common methods and properties, e.g., optimise that runs the optimisation loop until the chosen error, error_type , is small enough, opt_eps , and create_candidates that samples candidate points using Latin Hyperube sampling. The class can be inherited for implementing specific approaches. Currently waggon.optim contains surrogate-based optimisation, SurrogateOptimiser (described in the following section ). New methods will be added as our research continues and can be suggested via a pull request.","title":"Optimiser"},{"location":"optim/#description","text":"WAGGON focuses on optimisation methods, and Optimiser is a base class for optimisation algorithms. It contains common methods and properties, e.g., optimise that runs the optimisation loop until the chosen error, error_type , is small enough, opt_eps , and create_candidates that samples candidate points using Latin Hyperube sampling. The class can be inherited for implementing specific approaches. Currently waggon.optim contains surrogate-based optimisation, SurrogateOptimiser (described in the following section ). New methods will be added as our research continues and can be suggested via a pull request.","title":"Description"},{"location":"surr_opt/","text":"Description Surrogate-based optimisation (SBO) carries out optimisation using a surrogate model. SBO does not strive for global accuracy. Instead it only requires the surrogate model to be sufficiently accurate to guide optimisation toward the true optimum. SBO can be particularly useful in several scenarious. For example, when the original model is computationally expensive, or when data is noise, or both [1]. SurrogateOptimiser is based on the Optimiser class. To run, it requires the experiment, waggon.Function , the surrogate model, waggon.Surrogate , and the acquisition function, waggon.acquisition . Usage import waggon from waggon.acquisitions import WU from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel from waggon.optim import SurrogateOptimiser # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display() [1] Martins, J.R.R.A. and Ning, S.A. (2022) Surrogate-based Optimisation in Engineering design optimization . Cambridge, United Kingdom: Cambridge University Press.","title":"Surrogate based optimisation"},{"location":"surr_opt/#description","text":"Surrogate-based optimisation (SBO) carries out optimisation using a surrogate model. SBO does not strive for global accuracy. Instead it only requires the surrogate model to be sufficiently accurate to guide optimisation toward the true optimum. SBO can be particularly useful in several scenarious. For example, when the original model is computationally expensive, or when data is noise, or both [1]. SurrogateOptimiser is based on the Optimiser class. To run, it requires the experiment, waggon.Function , the surrogate model, waggon.Surrogate , and the acquisition function, waggon.acquisition .","title":"Description"},{"location":"surr_opt/#usage","text":"import waggon from waggon.acquisitions import WU from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel from waggon.optim import SurrogateOptimiser # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display() [1] Martins, J.R.R.A. and Ning, S.A. (2022) Surrogate-based Optimisation in Engineering design optimization . Cambridge, United Kingdom: Cambridge University Press.","title":"Usage"},{"location":"wu/","text":"TBD","title":"Wasserstein Uncertainty"}]}