{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to WAGGON: WAssrestein Global Gradient-free OptimisatioN WAGGON is a python library of black box gradient-free optimisation. Currently, the library contains implementations of optimisation methods based on Wasserstein uncertainty and baseline approaches from the following papers: Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv] Implemented methods Wasserstein Uncertainty Global Optimisation (WU-GO) Bayesian optimisation: via Expected Improvement (EI), Lower and Upper Confidence Bounds (LCB, UCB) Installation pip install waggon or git clone https://github.com/hse-cs/waggon cd waggon pip install -e Basic usage (See more examples in the documentation .) The following code snippet (does this and that) import waggon from waggon.acquisitions import WU from waggon.optim import SurrogateOptimiser from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display() Support Home: https://github.com/hse-cs/waggon Documentation: https://hse-cs.github.io/waggon For any usage questions, suggestions and bugs please use the issue page .","title":"Home"},{"location":"#welcome-to-waggon-wassrestein-global-gradient-free-optimisation","text":"WAGGON is a python library of black box gradient-free optimisation. Currently, the library contains implementations of optimisation methods based on Wasserstein uncertainty and baseline approaches from the following papers: Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv]","title":"Welcome to WAGGON: WAssrestein Global Gradient-free OptimisatioN"},{"location":"#implemented-methods","text":"Wasserstein Uncertainty Global Optimisation (WU-GO) Bayesian optimisation: via Expected Improvement (EI), Lower and Upper Confidence Bounds (LCB, UCB)","title":"Implemented methods"},{"location":"#installation","text":"pip install waggon or git clone https://github.com/hse-cs/waggon cd waggon pip install -e","title":"Installation"},{"location":"#basic-usage","text":"(See more examples in the documentation .) The following code snippet (does this and that) import waggon from waggon.acquisitions import WU from waggon.optim import SurrogateOptimiser from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display()","title":"Basic usage"},{"location":"#support","text":"Home: https://github.com/hse-cs/waggon Documentation: https://hse-cs.github.io/waggon For any usage questions, suggestions and bugs please use the issue page .","title":"Support"},{"location":"about/","text":"Black-box Optimisation Simulation of real-world experiments is key to scientific discoveries and engineering solutions. Such techniques use parameters describing configurations and architectures of the system. A common challenge is to find the optimal configuration for some objective, e.g., such that it maximises efficiency or minimises costs and overhead. The simulator is treated as a black box experiment. This means that observations come from an unknown and likely difficult-to-estimate function. Using a surrogate model for black-box optimisation (BBO) is an established technique, as BBO has a rich history of both gradient and gradient-free methods, most of which come from tackling problems that arise in physics, chemistry and engineering. The optimisation problem can be defined as: $$ \\inf_{\\theta \\in \\Theta} f(\\theta), $$ where $\\Theta$ is the search space. If the black-box function is stochastic, the problem accepts the following form: $$ \\inf_{\\theta \\in \\Theta} \\mathbb{E}\\left[ f(\\theta, x) \\right]. $$ And in case of distributionally robust optimistion, the objective becomes: $$ \\inf_{\\theta \\in \\Theta} \\sup_{\\mu \\in \\mathcal{P}} \\mathbb{E}_{x \\sim \\mu} \\left[ f(\\theta, x) \\right], $$ where $\\mathcal{P}$ is an ambiguity set, i.e., set of viable distributions.","title":"Black-box optimisation"},{"location":"about/#black-box-optimisation","text":"Simulation of real-world experiments is key to scientific discoveries and engineering solutions. Such techniques use parameters describing configurations and architectures of the system. A common challenge is to find the optimal configuration for some objective, e.g., such that it maximises efficiency or minimises costs and overhead. The simulator is treated as a black box experiment. This means that observations come from an unknown and likely difficult-to-estimate function. Using a surrogate model for black-box optimisation (BBO) is an established technique, as BBO has a rich history of both gradient and gradient-free methods, most of which come from tackling problems that arise in physics, chemistry and engineering. The optimisation problem can be defined as: $$ \\inf_{\\theta \\in \\Theta} f(\\theta), $$ where $\\Theta$ is the search space. If the black-box function is stochastic, the problem accepts the following form: $$ \\inf_{\\theta \\in \\Theta} \\mathbb{E}\\left[ f(\\theta, x) \\right]. $$ And in case of distributionally robust optimistion, the objective becomes: $$ \\inf_{\\theta \\in \\Theta} \\sup_{\\mu \\in \\mathcal{P}} \\mathbb{E}_{x \\sim \\mu} \\left[ f(\\theta, x) \\right], $$ where $\\mathcal{P}$ is an ambiguity set, i.e., set of viable distributions.","title":"Black-box Optimisation"},{"location":"bnn/","text":"Description Bayesian inference allows us to learn a probability distribution over possible neural networks. By making a simple adjustment to standard neural network methods, we can approximately solve this inference problem. The resulting approach reduces overfitting, facilitates learning from small datasets, and provides insights into the uncertainty of our predictions. In Bayesian inference, instead of relying on a single point estimate of the weights, $w^ $, and its associated prediction function, $\\hat{y}_{w^ }(x)$, as in conventional training, we infer distributions $p(w|D)$ and $p(\\hat{y}_{w^*}(x)|D)$. One key advantage of using distributions for model parameters and predictions is the ability to quantify uncertainty, such as by calculating the variance [3]. Usage from waggon.surrogates import BNN [3] Xu, W., Ricky, Li, X. and Duvenaud, D. (2022). Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations. PMLR, pp.721\u2013738. Available online . \u200c","title":"Bayesian Neural Networks"},{"location":"bnn/#description","text":"Bayesian inference allows us to learn a probability distribution over possible neural networks. By making a simple adjustment to standard neural network methods, we can approximately solve this inference problem. The resulting approach reduces overfitting, facilitates learning from small datasets, and provides insights into the uncertainty of our predictions. In Bayesian inference, instead of relying on a single point estimate of the weights, $w^ $, and its associated prediction function, $\\hat{y}_{w^ }(x)$, as in conventional training, we infer distributions $p(w|D)$ and $p(\\hat{y}_{w^*}(x)|D)$. One key advantage of using distributions for model parameters and predictions is the ability to quantify uncertainty, such as by calculating the variance [3].","title":"Description"},{"location":"bnn/#usage","text":"from waggon.surrogates import BNN [3] Xu, W., Ricky, Li, X. and Duvenaud, D. (2022). Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations. PMLR, pp.721\u2013738. Available online . \u200c","title":"Usage"},{"location":"cb/","text":"Description TBD Usage from waggon.acquisitions import LCB, UCB","title":"Confidence Bound"},{"location":"cb/#description","text":"TBD","title":"Description"},{"location":"cb/#usage","text":"from waggon.acquisitions import LCB, UCB","title":"Usage"},{"location":"custom_acqf/","text":"TBD","title":"Custom Acquisition Functions"},{"location":"custom_exp/","text":"TBD","title":"Custom experiments"},{"location":"custom_surr/","text":"TBD","title":"Custom Surrogates"},{"location":"de/","text":"Description An intuitive approach for uncertainty quantifi- cation is using an ensemble surrogate model, e.g., an adversarial deep ensemble (DE) [4]. Single predictors of the ensemble are expected to agree on their predictions over observed regions of the fea- ture space, i.e., where data are given and so the uncertainty is low and vice versa. The further these single predictors get from known regions of the feature space, the greater the discrepancy in their predictions. Usage from waggon.surrogates import DE [4] Lakshminarayanan, B., Pritzel, A. and Deepmind, C. (n.d.). Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. PMLR. Available at: https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf. \u200c","title":"Deep Adversarial Ensembles"},{"location":"de/#description","text":"An intuitive approach for uncertainty quantifi- cation is using an ensemble surrogate model, e.g., an adversarial deep ensemble (DE) [4]. Single predictors of the ensemble are expected to agree on their predictions over observed regions of the fea- ture space, i.e., where data are given and so the uncertainty is low and vice versa. The further these single predictors get from known regions of the feature space, the greater the discrepancy in their predictions.","title":"Description"},{"location":"de/#usage","text":"from waggon.surrogates import DE [4] Lakshminarayanan, B., Pritzel, A. and Deepmind, C. (n.d.). Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. PMLR. Available at: https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf. \u200c","title":"Usage"},{"location":"dgm/","text":"Description Having a generator model that would map a simple distribution, $\\mathcal{Z}$, to a complex and unknown distribution, $p(\\mu)$, is desired in many settings as it allows for the generation of samples from the intractable data space. For our purposes, conditional deep generative models (DGMs) are used to model the stochastic response of the simulator. This allows to model essentially any response shape. The goal of a conditional DGM is to obtain a generator $G: \\mathbb{R}^s \\times \\Theta \\rightarrow \\mathbb{R}^d$ such that distributions $G(\\mathcal{Z}; \\theta)$ and $p(\\mu; \\theta)$ match for each $\\theta \\in \\Theta$. Since $\\mathcal{Z}$ and $p(\\mu)$ are independent, we get $G(\\mathcal{Z}; \\theta) \\sim p(\\mu; \\theta)$. For simplicity of notation we denote $G(\\mathcal{Z}; \\theta)$ as $G(\\theta)$. As per [2], the package contains the implementation of a Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) to be used with Wasserstein uncertainty. Usage from waggon.surrogates.gan import WGAN [2] Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv]","title":"Deep Generative Surrogates"},{"location":"dgm/#description","text":"Having a generator model that would map a simple distribution, $\\mathcal{Z}$, to a complex and unknown distribution, $p(\\mu)$, is desired in many settings as it allows for the generation of samples from the intractable data space. For our purposes, conditional deep generative models (DGMs) are used to model the stochastic response of the simulator. This allows to model essentially any response shape. The goal of a conditional DGM is to obtain a generator $G: \\mathbb{R}^s \\times \\Theta \\rightarrow \\mathbb{R}^d$ such that distributions $G(\\mathcal{Z}; \\theta)$ and $p(\\mu; \\theta)$ match for each $\\theta \\in \\Theta$. Since $\\mathcal{Z}$ and $p(\\mu)$ are independent, we get $G(\\mathcal{Z}; \\theta) \\sim p(\\mu; \\theta)$. For simplicity of notation we denote $G(\\mathcal{Z}; \\theta)$ as $G(\\theta)$. As per [2], the package contains the implementation of a Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) to be used with Wasserstein uncertainty.","title":"Description"},{"location":"dgm/#usage","text":"from waggon.surrogates.gan import WGAN [2] Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv]","title":"Usage"},{"location":"dgp/","text":"Description Deep Gaussian processes (DGP) [5] attempt to resolve the issue of finding the best kernel for a GP. That is done by stacking GPs in a hierarchical struc- ture as Perceptrons in a multilayer perceptron, but the number of variational parameters to be learnt by DGPs increases linearly with the number of data points, which is infeasible for stochastic black-box optimisation, and they have the same matrix inverting issue as GPs, which limits their scalability. Usage from waggon.surrogates import DGP [5\u200c] Damianou, A. and Lawrence, N. (2013). Deep Gaussian Processes. PMLR. [online] Available at: https://proceedings.mlr.press/v31/damianou13a.pdf \u200c","title":"Deep Gaussian Processes"},{"location":"dgp/#description","text":"Deep Gaussian processes (DGP) [5] attempt to resolve the issue of finding the best kernel for a GP. That is done by stacking GPs in a hierarchical struc- ture as Perceptrons in a multilayer perceptron, but the number of variational parameters to be learnt by DGPs increases linearly with the number of data points, which is infeasible for stochastic black-box optimisation, and they have the same matrix inverting issue as GPs, which limits their scalability.","title":"Description"},{"location":"dgp/#usage","text":"from waggon.surrogates import DGP [5\u200c] Damianou, A. and Lawrence, N. (2013). Deep Gaussian Processes. PMLR. [online] Available at: https://proceedings.mlr.press/v31/damianou13a.pdf \u200c","title":"Usage"},{"location":"ei/","text":"Description TBD Usage from waggon.acquisitions import EI","title":"Expected Improvement"},{"location":"ei/#description","text":"TBD","title":"Description"},{"location":"ei/#usage","text":"from waggon.acquisitions import EI","title":"Usage"},{"location":"functions/","text":"Description Currently, the following test functions, commonly used in optimisation, are implemented: - Three Hump Camel - Rosenbrock - Ackley - Himmelblau - L\u00e9vi - Styblinksi-Tang - H\u00f6lder Usage from waggon.functions import three_hump_camel from waggon.functions import rosenbrock from waggon.functions import ackley from waggon.functions import himmelblau from waggon.functions import levi from waggon.functions import tang from waggon.functions import holder","title":"Test functions"},{"location":"functions/#description","text":"Currently, the following test functions, commonly used in optimisation, are implemented: - Three Hump Camel - Rosenbrock - Ackley - Himmelblau - L\u00e9vi - Styblinksi-Tang - H\u00f6lder","title":"Description"},{"location":"functions/#usage","text":"from waggon.functions import three_hump_camel from waggon.functions import rosenbrock from waggon.functions import ackley from waggon.functions import himmelblau from waggon.functions import levi from waggon.functions import tang from waggon.functions import holder","title":"Usage"},{"location":"gp/","text":"Description Bayesian optimisation is typically equipped with a Gaussian process (GP), which is defined by a mean function and a kernel function that describes the shape of the covariance. BO with a GP surrogate requires covariance matrix inversion with $O(n^3)$ cost in terms of the number of observations, which is challenging for a large number of responses and in higher dimensions. To make BO scalable [6, 7, 8, 9] consider a low-dimensional lin- ear subspace and decompose it into subsets of dimensions, i.e., some structural assumptions are required that might not hold. In addition, GP requires a proper choice of kernel is crucial. BO may need the construction of new kernels [10]. [11] proposes a greedy approach for automatically building a kernel by combining basic kernels such as linear, exponential, periodic, etc., through kernel summation and multiplication. Usage from waggon.surrogates import GP [6] N.deFreitasandZ.Wang.Bayesian optimization in high dimensions via random embeddings. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, 2013. \u200c [7] J. Djolonga, A. Krause, and V. Cevher. High-dimensional gaussian process bandits. Advances in neural information processing systems, 26, 2013. [8] R. Garnett, M. A. Osborne, and P. Hennig. Active learning of linear embeddings for gaussian processes. arXiv preprint arXiv:1310.6740, 2013. [9] M. Zhang, H. Li, and S. Su. High dimensional bayesian optimization via supervised dimension reduction. arXiv preprint arXiv:1907.08953, 2019. [10] N. S. Gorbach, A. A. Bian, B. Fischer, S. Bauer, and J. M. Buhmann. Model selection for gaussian process regression. In Pattern Recognition: 39th German Conference, GCPR 2017, Basel, Switzerland, September 12\u201315, 2017, Proceedings 39, pages 306\u2013318. Springer, 2017. [11] D. Duvenaud. Automatic model construction with Gaussian processes. PhD thesis, University of Cambridge, 2014.","title":"Gaussian Processes"},{"location":"gp/#description","text":"Bayesian optimisation is typically equipped with a Gaussian process (GP), which is defined by a mean function and a kernel function that describes the shape of the covariance. BO with a GP surrogate requires covariance matrix inversion with $O(n^3)$ cost in terms of the number of observations, which is challenging for a large number of responses and in higher dimensions. To make BO scalable [6, 7, 8, 9] consider a low-dimensional lin- ear subspace and decompose it into subsets of dimensions, i.e., some structural assumptions are required that might not hold. In addition, GP requires a proper choice of kernel is crucial. BO may need the construction of new kernels [10]. [11] proposes a greedy approach for automatically building a kernel by combining basic kernels such as linear, exponential, periodic, etc., through kernel summation and multiplication.","title":"Description"},{"location":"gp/#usage","text":"from waggon.surrogates import GP [6] N.deFreitasandZ.Wang.Bayesian optimization in high dimensions via random embeddings. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, 2013. \u200c [7] J. Djolonga, A. Krause, and V. Cevher. High-dimensional gaussian process bandits. Advances in neural information processing systems, 26, 2013. [8] R. Garnett, M. A. Osborne, and P. Hennig. Active learning of linear embeddings for gaussian processes. arXiv preprint arXiv:1310.6740, 2013. [9] M. Zhang, H. Li, and S. Su. High dimensional bayesian optimization via supervised dimension reduction. arXiv preprint arXiv:1907.08953, 2019. [10] N. S. Gorbach, A. A. Bian, B. Fischer, S. Bauer, and J. M. Buhmann. Model selection for gaussian process regression. In Pattern Recognition: 39th German Conference, GCPR 2017, Basel, Switzerland, September 12\u201315, 2017, Proceedings 39, pages 306\u2013318. Springer, 2017. [11] D. Duvenaud. Automatic model construction with Gaussian processes. PhD thesis, University of Cambridge, 2014.","title":"Usage"},{"location":"optim/","text":"Description WAGGON focuses on optimisation methods, and Optimiser is a base class for optimisation algorithms. It contains common methods and properties, e.g., optimise that runs the optimisation loop until the chosen error, error_type , is small enough, opt_eps , and create_candidates that samples candidate points using Latin Hyperube sampling. The class can be inherited for implementing specific approaches. Currently waggon.optim contains surrogate-based optimisation, SurrogateOptimiser (described in the following section ). New methods will be added as our research continues and can be suggested via a pull request. Usage from waggon.optim import Optimiser","title":"Optimiser"},{"location":"optim/#description","text":"WAGGON focuses on optimisation methods, and Optimiser is a base class for optimisation algorithms. It contains common methods and properties, e.g., optimise that runs the optimisation loop until the chosen error, error_type , is small enough, opt_eps , and create_candidates that samples candidate points using Latin Hyperube sampling. The class can be inherited for implementing specific approaches. Currently waggon.optim contains surrogate-based optimisation, SurrogateOptimiser (described in the following section ). New methods will be added as our research continues and can be suggested via a pull request.","title":"Description"},{"location":"optim/#usage","text":"from waggon.optim import Optimiser","title":"Usage"},{"location":"surr_opt/","text":"Description Surrogate-based optimisation (SBO) carries out optimisation using a surrogate model. SBO does not strive for global accuracy. Instead it only requires the surrogate model to be sufficiently accurate to guide optimisation toward the true optimum. SBO can be particularly useful in several scenarious. For example, when the original model is computationally expensive, or when data is noise, or both [1]. SurrogateOptimiser is based on the Optimiser class. To run, it requires the experiment, waggon.Function , the surrogate model, waggon.Surrogate , and the acquisition function, waggon.acquisition . Usage import waggon from waggon.acquisitions import WU from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel from waggon.optim import SurrogateOptimiser # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display() [1] Martins, J.R.R.A. and Ning, S.A. (2022) Surrogate-based Optimisation in Engineering design optimization . Cambridge, United Kingdom: Cambridge University Press.","title":"Surrogate based optimisation"},{"location":"surr_opt/#description","text":"Surrogate-based optimisation (SBO) carries out optimisation using a surrogate model. SBO does not strive for global accuracy. Instead it only requires the surrogate model to be sufficiently accurate to guide optimisation toward the true optimum. SBO can be particularly useful in several scenarious. For example, when the original model is computationally expensive, or when data is noise, or both [1]. SurrogateOptimiser is based on the Optimiser class. To run, it requires the experiment, waggon.Function , the surrogate model, waggon.Surrogate , and the acquisition function, waggon.acquisition .","title":"Description"},{"location":"surr_opt/#usage","text":"import waggon from waggon.acquisitions import WU from waggon.surrogates.gan import WGAN_GP as GAN from waggon.test_functions import three_hump_camel from waggon.optim import SurrogateOptimiser # initialise the function to be optimised func = three_hump_camel() # initialise the surrogate to carry out optimisation surr = GAN() # initialise optimisation acquisition function acqf = WU() # initialise optimiser opt = SurrogateOptimiser(func=func, surr=surr, acqf=acqf) # run optimisation opt.optimise() # visualise optimisation results waggon.utils.display() [1] Martins, J.R.R.A. and Ning, S.A. (2022) Surrogate-based Optimisation in Engineering design optimization . Cambridge, United Kingdom: Cambridge University Press.","title":"Usage"},{"location":"wu/","text":"Description As per Ref.[2] Usage from waggon.acquisitions import WU [2] Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv]","title":"Wasserstein Uncertainty"},{"location":"wu/#description","text":"As per Ref.[2]","title":"Description"},{"location":"wu/#usage","text":"from waggon.acquisitions import WU [2] Tigran Ramazyan, Mikhail Hushchyn and Denis Derkach. \"Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space.\" Arxiv abs/2407.1117 (2024). [arxiv]","title":"Usage"}]}